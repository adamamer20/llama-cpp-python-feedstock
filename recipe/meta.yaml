{% set name = "llama-cpp-python" %}
{% set version = "0.2.20" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/llama_cpp_python-{{ version }}.tar.gz
  sha256: a0ada1cb800ba4da60ea6ac4f7264b687a35412374e5af2c92e5b22852cdbafb
  patches:
    # https://github.com/abetlen/llama-cpp-python/commit/e3941d9c674dbd9891dc3ceda390daeb21f05fd1
    - e3941d9.patch 
    - osx-64-pick-discrete.patch  # [osx]
    - mkl.patch                   # [blas_impl == "mkl"]

build:
  number: 6
  string: cuda{{ cuda_compiler_version | replace('.', '') }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}  # [cuda_compiler_version != "None"]
  string: cpu_{{ blas_impl }}_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                 # [(osx and x86_64) or cuda_compiler_version == "None"]
  string: mps_h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}                                                 # [osx and arm64]
  script:
    {% macro cmake_args(key, value) -%}
    - export CMAKE_ARGS="${CMAKE_ARGS} -D{{ key }}={{ value }}"    # [unix]
    - set CMAKE_ARGS=%CMAKE_ARGS% -D{{ key }}={{ value }}          # [win]
    {%- endmacro %}
    {% macro llama_args(key, value) -%}
    {{ cmake_args("LLAMA_"+key, value) }}
    {%- endmacro %}

    {{ cmake_args("LLAVA_BUILD", "ON") }}

    {{ llama_args("NATIVE", "OFF") }}        # [(osx and arm64) or win]
    {{ llama_args("ACCELERATE", "ON") }}     # [osx]
    {{ llama_args("METAL", "ON") }}          # [osx and arm64]
    {{ llama_args("METAL", "OFF") }}         # [osx and x86_64]
    {{ llama_args("CUBLAS", "ON") }}         # [cuda_compiler_version != "None"]

    {{ llama_args("BLAS", "ON") }}                        # [not osx and cuda_compiler_version == "None"]
    {{ llama_args("BLAS_VENDOR", "Intel10_64_dyn") }}     # [blas_impl == "mkl"]

    - export CMAKE_GENERATOR=Ninja    # [unix]
    - set CMAKE_GENERATOR=Ninja       # [win]
    - set CMAKE_GENERATOR_PLATFORM=   # [win]
    - set CMAKE_GENERATOR_TOOLSET=    # [win]

    - {{ PYTHON }} -m pip install . -vv

requirements:
  build:
    - python                                 # [build_platform != target_platform]
    - cross-python_{{ target_platform }}     # [build_platform != target_platform]

    # llama.cpp deps
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}                 # [cuda_compiler_version != "None"]
    - cmake
    - git
    - ninja
    # Required to find mkl on windows
    - pkgconfig  # [blas_impl == "mkl"]
  host:
    - python
    - scikit-build-core >=0.5.1
    - pip

    # llama.cpp deps
    - cuda-cudart-dev                # [(cuda_compiler_version or "").startswith("12")]
    - libcublas-dev                  # [(cuda_compiler_version or "").startswith("12")]

    - blas-devel * *{{ blas_impl }}  # [blas_impl == "mkl"]
    - mkl-devel {{ mkl }}            # [blas_impl == "mkl"]
  run:
    - python
    - typing-extensions >=4.5.0
    - numpy >=1.20.0
    - diskcache >=5.6.1
    - uvicorn >=0.22.0
    - fastapi >=0.100.0
    - pydantic-settings >=2.0.1
    - sse-starlette >=1.6.1
    - starlette-context >=0.3.6,<0.4

    # llama.cpp deps
    - cuda-version {{ cuda_compiler_version }}   # [cuda_compiler_version != "None"]
    - cuda-cudart  {{ cuda_compiler_version }}   # [(cuda_compiler_version or "").startswith("12")]
    - cuda-cudart_{{ target_platform }} {{ cuda_compiler_version }}  # [(cuda_compiler_version or "").startswith("12") and win]
test:
  imports:
    - llama_cpp
  commands:
    - pip check
  requires:
    - pip

about:
  home: https://github.com/abetlen/llama-cpp-python
  summary: Python bindings for the llama.cpp library
  license: MIT
  license_file:
    - LICENSE.md
    - vendor/llama.cpp/LICENSE
    - vendor/llama.cpp/gguf-py/LICENSE

extra:
  recipe-maintainers:
    - YYYasin19
    - sodre
